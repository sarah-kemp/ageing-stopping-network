{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd92dc6-29d7-4b6a-9e7d-0bfe054cc6da",
   "metadata": {},
   "source": [
    "## Stopping the AHEAD\n",
    "    Original code by:  Max Keuken (mckeuken@gmail.com)\n",
    "                       Integrative Model-based Cognitive Neuroscience \n",
    "                       (IMCN) research unit, University of Amsterdam\n",
    "    Date:              Feb 2021\n",
    "    Adapted by:        Sarah Kemp (sarahannekemp@gmail.com)\n",
    "                       Pilou Bazin (p.l.e.a.bazin@uva.nl)\n",
    "    Date:              2022\n",
    "    \n",
    "##### Description of the project\n",
    "    The 3T DWI scans have been preprocessed using the notebook '1_Preprocessing-script-AHEAD_DWI-Notebook.ipynb'. \n",
    "    Steps like denoising, degibbing, eddy correction and bias field correction have all be applied to the data. \n",
    "    \n",
    "    For a project together with Andrew Heathcote, Mark Hinder, Dora Matze, Sarah Kemp, Niek Stevenson, Pilou Bazin     \n",
    "    and Birte Forstmann we want to reinvite the AHEAD subjects for a stop related task and link the \n",
    "    behavioral SSRT model parameters to the connectome of these subjects. The behavioral data ended up being not very \n",
    "    usefull as a large number of subjects did not seem to follow the task instructions. So we decided to use the data\n",
    "    to determine how many new subjects we would need to test if we want to find a robust correlation between \n",
    "    age and tractstrength. Once we know this we can determine if the project is even remotely feasible. \n",
    "    \n",
    "    We basically need to have the tract strengths per subject so that we can sample from it and correlate age \n",
    "    with tract strength and determine the required sample size to have a correlation that is associated with \n",
    "    a bayes factor of atleast 10.\n",
    "    \n",
    "    Given the quality of the data we can use spherical deconvolution but we chose not to use processing steps\n",
    "    such as Anatomical Constrained Tractography (ACT) as most of the subcortex is missing as priors in this\n",
    "    pipeline. We tried to keep it as straightforward as possible. \n",
    "    \n",
    "    \n",
    "\n",
    "##### Description of the pipeline\n",
    "- [X] 0) read the demographics and get the DWI data\n",
    "- [X] 1) create a basis function (dwi2response)\n",
    "- [X] 2) Estimate the Fiber Orientation Density (FOD) and the global tractography using tckglobal \n",
    "- [X] 3) Refining the streamlining using SIFT2\n",
    "- [X] 4) Create the ROIs for tracking\n",
    "- [X] 5) Estimate the connectome for my cortical and subcortical areas of interest \n",
    "- [X] 6) Extract the streamlines between nodes for visual inspectation\n",
    "- [X] 7) Create a dataframe with the streamcounts for all subjects\n",
    "- [X] 8) Register the B0's to MNI space and apply to mean tracks\n",
    "- [X] Did you do all the quality control checks??\n",
    "    \n",
    "##### Software versions\n",
    "    mrtrix V3.0.2\n",
    "    fsl V5.0.11\n",
    "    ants via python (antspyx V0.2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa610c-f868-4ce9-a06f-7654ee7a383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## Import the modules\n",
    "#################################################################################\n",
    "import pandas as pd, numpy as np, glob, re, seaborn as sns, matplotlib.pyplot as plt\n",
    "import shutil, subprocess, sys, os, ants, nibabel as nib\n",
    "\n",
    "from multiprocessing import Pool\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ceb5c-508f-4166-bebd-ee2e31ecfdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 0) Variables, demographics and dwi data\n",
    "#################################################################################\n",
    "# Path settings\n",
    "mainProjectFolder = '/home/public/StoppingAge-DWI-Ahead/'\n",
    "demographicsDataFolder = 'Data/Demographics/'\n",
    "dwiDataFolder = 'Data/Dwi/'\n",
    "dwiDerivativeFolder = 'Data/Derivatives/'\n",
    "preprocessedDataFolder = '/home/public/HumanAtlas_BIDS/'\n",
    "    \n",
    "# Read the demographic data \n",
    "dfDemographic = pd.read_csv(mainProjectFolder+demographicsDataFolder+'StoppingAge-DWI-AHEAD-Demographics-ALL-DWI.csv')\n",
    "\n",
    "# The names of the subjects are not however in the BIDS format. So lets convert them:\n",
    "# Note that I'm not using the subjectID codes as the anatomy of the 7T scans in the HumanAtlas_BIDS folder \n",
    "#. would then not match the 3T DWI scans. I.e. scancode '0017' matches the anatomy of sub-017 bids format.\n",
    "subjects_bidsTmp = [item.lower() for item in dfDemographic['Scan_Code']]\n",
    "subjects_bids = []\n",
    "for i in range(len(subjects_bidsTmp)):\n",
    "    if 'pilot' in subjects_bidsTmp[i]:\n",
    "        bidsName = 'sub-'+'000'\n",
    "    else:\n",
    "        bidsName = 'sub-'+subjects_bidsTmp[i][11:14]\n",
    "    subjects_bids.append(bidsName)\n",
    "    \n",
    "# Add them to the dataframe:    \n",
    "dfDemographic['Subject-BIDS'] = subjects_bids\n",
    "subjects = dfDemographic['Subject-BIDS']\n",
    "\n",
    "print('How many DWI subjects do we?', len(subjects))\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab4856-5800-465f-ba41-b6c7ca6cc66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 1) Create basis function\n",
    "#################################################################################\n",
    "# We are going to use the dhollander algorithm as this seems to be the preferred option \n",
    "#. with multi shell data.\n",
    "\n",
    "# Done for all 49 subjects? Yes\n",
    "\n",
    "overwrite = False\n",
    "def createBasisFunction(subject):\n",
    "    print(subject+'\\n')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    dwiresponse_dir = work_dir+'/dwi2response'\n",
    "   \n",
    "    # Location of the original dwi data\n",
    "    origDWIData = preprocessedDataFolder+subject+'/ses-3/dwi/'+subject+'_ses-3_dwi-preproc.mif.gz'\n",
    "    origDWIMask = preprocessedDataFolder+subject+'/ses-3/dwi/'+subject+'_ses-3_dwi_mask.mif.gz'\n",
    "    \n",
    "    # Location of the local copy\n",
    "    inputData = preprocessed_dir+'/'+subject+'_ses-3_dwi-preproc.mif.gz'\n",
    "    brainMask = preprocessed_dir+'/'+subject+'_ses-3_dwi_mask.mif.gz'\n",
    "    \n",
    "    # Create the output folder\n",
    "    if ((os.path.exists(dwiresponse_dir)) & (overwrite == False)):\n",
    "        print('\\n1. Estimate Basis function: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n1. Estimate Basis function')\n",
    "        os.makedirs(preprocessed_dir,exist_ok=True)\n",
    "        os.makedirs(dwiresponse_dir,exist_ok=True)\n",
    "        \n",
    "        # Make a local copy of the original data\n",
    "        shutil.copy(origDWIData, inputData)\n",
    "        shutil.copy(origDWIMask, brainMask)\n",
    "        \n",
    "        # Estimate the basis function\n",
    "        #. Note that we are providing our own initial mask that we then strongly erode to ensure\n",
    "        #. that the eyeballs are not included to estimate the csf and we stay clearly away from \n",
    "        #. the cortical outer edge\n",
    "        command = 'dwi2response dhollander '+inputData+' '+dwiresponse_dir+'/wm.txt '+dwiresponse_dir+'/gm.txt '+dwiresponse_dir+'/csf.txt -mask '+brainMask+' -voxels '+dwiresponse_dir+'/voxels.mif -erode 15 -force'\n",
    "\n",
    "        print(command)\n",
    "        try:\n",
    "             subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "# Run the actual function     \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(4) # 3 threads \n",
    "    pool.imap(createBasisFunction, subjects) \n",
    "    pool.close()       \n",
    "\n",
    "# Check a few things:\n",
    "# 1) are the voxels that are used to estimate the response in a correct area (ie csf voxels in csf, wm voxels in wm etc)\n",
    "#.   you can check this by looking at the voxels.mif on top of the mean BO\n",
    "# 2) are the response functions as expected and do they change as you would expect with a change in B values?\n",
    "#.   you can check this by looking at the text files with shview gm.txt etc\n",
    "#. This all seems to be the case.\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733ab3d-6557-4daf-abdb-073080023ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 2) Estimate the Fiber Orientation Density (FOD) using dwi2fod\n",
    "#################################################################################\n",
    "# This was initially done using tckglobal, but there were a number of participants for whom there were very\n",
    "# few streamlines. Therefore, later we reran the FOD and global tractography using dwi2fod and tckgen \n",
    "# (rather than tckglobal).\n",
    "\n",
    "# Do you want to overwrite the previous results? If yes, then True\n",
    "# Done for all 49 subjects? Yes\n",
    "\n",
    "overwrite = False\n",
    "\n",
    "def dwi2fod(subject):\n",
    "    print(subject+'\\n')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = mainProjectFolder+'Data/Dwi/'+subject+'/preprocessed'\n",
    "    dwifod_dir = work_dir+'/dwi2fod'\n",
    "    dwiresponse_dir = mainProjectFolder+'Data/Dwi/'+subject+'/dwi2response'\n",
    "    \n",
    "    # Location of the local copy\n",
    "    inputData = preprocessed_dir+'/'+subject+'_ses-3_dwi-preproc.mif.gz'\n",
    "    brainMask = preprocessed_dir+'/'+subject+'_ses-3_dwi_mask.mif.gz'\n",
    "    \n",
    "    # Create the output folder\n",
    "    if ((os.path.exists(dwifod_dir)) & (overwrite == False)):\n",
    "        print('\\n2. Estimate FOD and run global tractography: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n2. Estimate FOD and run global tractography')\n",
    "        os.makedirs(dwifod_dir,exist_ok=True)\n",
    "        \n",
    "        # Estimate the Fiber Orientation Density (FOD)\n",
    "        command = 'dwi2fod msmt_csd '+inputData+' '+dwiresponse_dir+'/wm.txt '+dwifod_dir+'/wmfod.mif '+dwiresponse_dir+'/gm.txt '+dwifod_dir+'/gmfod.mif '+dwiresponse_dir+'/csf.txt '+dwifod_dir+'/csffod.mif  -nthreads 4'\n",
    "        \n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "             subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "# Run the actual function with pool  \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(4) # n threads \n",
    "    pool.map(dwi2fod, subjects) \n",
    "    pool.close() \n",
    "\n",
    "\n",
    "# Alternatively can just loop through (takes longer):\n",
    "# for subject in subjects: \n",
    "#     dwi2fod(subject)\n",
    "\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730f101-0fcb-4944-a0c3-007d12aef11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 3) Create tracks using the FODs just generated via dwi2fod\n",
    "#################################################################################\n",
    "#. Using the resulting FOD's we can use tckgen to generate tracks\n",
    "\n",
    "# Do you want to overwrite the previous results? If yes, then True\n",
    "\n",
    "# Done for all 49 subjects? Yes\n",
    "\n",
    "overwrite = False\n",
    "\n",
    "def wbStreamLineTckGen(subject):\n",
    "    print(subject+'\\n')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    dwiresponse_dir = work_dir+'/dwi2response'\n",
    "    wholebrainStreamline_dir = work_dir+'/tckgen'\n",
    "    \n",
    "    # Location of the local copy\n",
    "    inputData = preprocessed_dir+'/'+subject+'_ses-3_dwi-preproc.mif.gz'\n",
    "    brainMask = preprocessed_dir+'/'+subject+'_ses-3_dwi_mask.mif.gz'\n",
    "    midlineMask = preprocessed_dir+'/midline_mask_b0.nii.gz'\n",
    "    \n",
    "    # Create the output folder\n",
    "    if ((os.path.exists(wholebrainStreamline_dir)) & (overwrite == False)):\n",
    "        print('\\n2. Whole brain streamline tckgen: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n2. Estimate whole brain streamlines tckgen')\n",
    "        os.makedirs(wholebrainStreamline_dir, exist_ok=True)\n",
    "        \n",
    "        # Run the streamlines using the FOD of tckglobal and the iFOD2 algo in tckgen\n",
    "        #. We also provide a midline exclusion mask that was made in fslview using the mean B0 image\n",
    "        #. In terms of other parameters than standard: way more streamlines (25mil)\n",
    "        command = 'tckgen '+work_dir+'/dwi2fod/wmfod.mif '+wholebrainStreamline_dir+'/tracks_25M.tck -seed_image '+brainMask+' -mask '+brainMask+' -exclude '+midlineMask+' -nthreads 4 -algorithm iFOD2 -select 25000000 -force -cutoff 0.05'\n",
    "\n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "             subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "     \n",
    "        # To be able to control the output we are going to subsample:\n",
    "        command = 'tckedit '+wholebrainStreamline_dir+'/tracks_25M.tck -number 200k '+wholebrainStreamline_dir+'/smallerTracks_200k.tck -force'\n",
    "        \n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "             subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "# Based on the cpu use you probably don't want to run more than 5 people at a time\n",
    "\n",
    "# Run the actual function   \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(4) # n threads \n",
    "    pool.map(wbStreamLineTckGen, subjects) \n",
    "    pool.close() \n",
    "    \n",
    "# Check a few things:\n",
    "#. Does the FOD's look oke? ie elongated on the CC? clear ascending tracts? color coding correct?\n",
    "#. Yes that all seem to be the case\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5761c0c-5b1d-46f4-9a5f-84e9bb82e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 3.fix) Create tracks using the FODs just generated via dwi2fod\n",
    "#################################################################################\n",
    "#. Using the resulting FOD's we can use tckgen to generate tracks\n",
    "\n",
    "# Here we run the extra fibers for inter-hemispherical connections and merge the results\n",
    "\n",
    "# Do you want to overwrite the previous results? If yes, then True\n",
    "\n",
    "# Done for all 49 subjects? Yes\n",
    "\n",
    "overwrite = False\n",
    "\n",
    "def wbStreamLineTckGenHemi(subject):\n",
    "    print(subject+'\\n')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    dwiresponse_dir = work_dir+'/dwi2response'\n",
    "    wholebrainStreamline_dir = work_dir+'/tckgen'\n",
    "    \n",
    "    # Location of the local copy\n",
    "    inputData = preprocessed_dir+'/'+subject+'_ses-3_dwi-preproc.mif.gz'\n",
    "    brainMask = preprocessed_dir+'/'+subject+'_ses-3_dwi_mask.mif.gz'\n",
    "    midlineMask = preprocessed_dir+'/midline_mask_b0.nii.gz'\n",
    "    \n",
    "    # Create the output folder\n",
    "    output = wholebrainStreamline_dir+'/tracks_combined.tck'\n",
    "    if ((os.path.exists(output)) & (overwrite == False)):\n",
    "        print('\\n2. Whole brain streamline tckgen: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n2. Estimate whole brain streamlines tckgen')\n",
    "        os.makedirs(wholebrainStreamline_dir, exist_ok=True)\n",
    "        \n",
    "        # Run the command for the inter-hemispheric separation only\n",
    "        # To fix the fact we had the exclusion step before :(\n",
    "        # 5M additional streamlines seem to make the streamline weights converge in testing\n",
    "        command = 'tckgen '+work_dir+'/dwi2fod/wmfod.mif '+wholebrainStreamline_dir+'/tracks_5M.tck -seed_image '+midlineMask+' -mask '+brainMask+' -include '+midlineMask+' -nthreads 4 -algorithm iFOD2 -select 5000000 -force -cutoff 0.05'\n",
    "\n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "     \n",
    "    output = wholebrainStreamline_dir+'/tracks_combined.tck'\n",
    "    if ((os.path.exists(output)) & (overwrite == False)):\n",
    "        print('\\n2b. Whole brain streamline merging: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n2b. Merge whole brain streamlines')\n",
    "        \n",
    "        # merge both files\n",
    "        command = 'tckedit '+wholebrainStreamline_dir+'/tracks_25M.tck '+wholebrainStreamline_dir+'/tracks_5M.tck '+wholebrainStreamline_dir+'/tracks_combined.tck -force'\n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # To be able to control the output we are going to subsample:\n",
    "        command = 'tckedit '+wholebrainStreamline_dir+'/tracks_combined.tck -number 200k '+wholebrainStreamline_dir+'/smallerTracks_200k.tck -force'\n",
    "        \n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "# Based on the cpu use you probably don't want to run more than 5 people at a time\n",
    "\n",
    "# Run the actual function   \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(4) # n threads \n",
    "    pool.map(wbStreamLineTckGenHemi, subjects) \n",
    "    pool.close() \n",
    "    \n",
    "# Check a few things:\n",
    "#. Does the FOD's look oke? ie elongated on the CC? clear ascending tracts? color coding correct?\n",
    "#. Yes that all seem to be the case\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc9025-87d1-4d5d-97fd-287a84b9e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 4) Midway sanity check: check number of streamlines generated \n",
    "#################################################################################\n",
    "\n",
    "# This module uses tckstats (from MRtrix 3.0) to count the number of streamlines that were \n",
    "# generated per person in the previous module in the \"command = tckgen...\" line. \n",
    "# If this number is something other than 25 million (or whatever number was specified, that\n",
    "# indicates that the process might have been interrupted and should be rerun (e.g. if you \n",
    "# ran out of diskspace or if the server was restarted).\n",
    "\n",
    "\n",
    "def checkStreamlines(subject):\n",
    "    \n",
    "    # File just generated by the previous module is here\n",
    "    tckFolder = mainProjectFolder+'Data/Dwi/'+subject+'/tckgen/'\n",
    "    \n",
    "\n",
    "    # Bash command to count the number of streamlines \n",
    "    command = 'tckstats '+tckFolder+'tracks_combined.tck -output count' \n",
    "    print(command+'\\n')\n",
    "\n",
    "    # run the command and print out the result \n",
    "    subprocess.run(command, shell=True) \n",
    "\n",
    "        \n",
    "# Run the actual function   \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(4) # n threads \n",
    "    pool.map(checkStreamlines, subjects) \n",
    "    pool.close() \n",
    "    \n",
    "#####\n",
    "# 25mil generated for each person so looks good to proceed\n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7a74a-a6d1-4c10-ac97-1a2d3dfbe783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 5) refining the streamlinings of tckgen while using the fod from tckglobal using SIFT2\n",
    "#################################################################################\n",
    "# Do you want to overwrite the previous results? If yes, then True\n",
    "# Done for all 49 subjects? Yes\n",
    "overwrite = False\n",
    "\n",
    "def sift2Refining(subject):    \n",
    "    print(subject+'\\n')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    dwiresponse_dir = work_dir+'/dwi2response'\n",
    "    wholebrainStreamline_dir = work_dir+'/tckgen'\n",
    "    fod_dir = work_dir+'/dwi2fod'\n",
    "    sift2_dir = work_dir+'/SIFT2-tckgen'\n",
    "    \n",
    "    # Create the output folder\n",
    "    if ((os.path.exists(sift2_dir)) & (overwrite == False)):\n",
    "        print('\\n3. Refine the streamlines from tckgen using SIFT2: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n3. Refine the streamlines from tckgen using SIFT2')\n",
    "        os.makedirs(sift2_dir,exist_ok=True)\n",
    "\n",
    "        # Clean up the streamlines\n",
    "        # standard sift2 algoritm\n",
    "        command = 'tcksift2 -out_mu '+sift2_dir+'/sift_mu.txt -out_coeffs '+sift2_dir+'/sift_coeffs.txt -nthreads 4 '+wholebrainStreamline_dir+'/tracks_combined.tck '+fod_dir+'/wmfod.mif '+sift2_dir+'/sift.txt -force'\n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "             subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "\n",
    "# Run the actual function   \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(1) \n",
    "    pool.map(sift2Refining, subjects) \n",
    "    pool.close() \n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a4a71-1bfb-4c50-90dc-a0ad93870f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 6) create the ROIs for tracking\n",
    "#################################################################################\n",
    "# We need an input file for the connectome that has the bilateral 3 ROI's:\n",
    "#.   STN, IFG, preSMA\n",
    "#.   The STN are based on MASSP, the IFG and preSMA are based on the masks of Rogier Mars \n",
    "#. \n",
    "# We need to do a few things:\n",
    "#.  1. Get the IFG and preSMA MNI masks registered to DWI space\n",
    "#.  2a. Threshold the STN masks to ensure that we only select the voxels that MASSP is pretty certain about (robust range higher than 20% of probability)\n",
    "#.  2b. Get the thresholded STN masks to DWI space, binarize them\n",
    "#.  3. Combine the STN, IFG and preSMA masks into a single nifti file\n",
    "#. \n",
    "#. All registrations will be visually checked to see if the masks end up where they should be. \n",
    "#. A few of them will need some manual tweaking in terms of parameters. \n",
    "#. \n",
    "#. Landmarks that were mainly used to check for allignment were the corpus callosum, pons, and ventricles \n",
    "#. \n",
    "# Done for all 49 subjects? \n",
    "###############################\n",
    "# 1) MNI masks to B0 space\n",
    "###############################\n",
    "# subjects_extra = ['sub-008', 'sub-009', 'sub-010', 'sub-011', 'sub-012', 'sub-018', 'sub-023', 'sub-031', \n",
    "#                 'sub-032', 'sub-033', 'sub-041', 'sub-044', 'sub-053', 'sub-054', 'sub-056', 'sub-058', \n",
    "#                 'sub-059', 'sub-062', 'sub-063', 'sub-064', 'sub-065', 'sub-069', 'sub-070',  'sub-078',\n",
    "#                 'sub-082', 'sub-083', 'sub-084', 'sub-089', 'sub-095']\n",
    "for subject in subjects:\n",
    "    print(subject+' MNI Masks')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    \n",
    "    # Location of the MNI template and masks data:\n",
    "    mniTemplate1mmBrain = '/home/public/StoppingAge-DWI-Ahead/Data/Atlases/MNI152_T1_1mm_brain.nii.gz'\n",
    "    mniIFG_l = '/home/public/StoppingAge-DWI-Ahead/Data/Atlases/mni_ifg_l_bin.nii.gz'\n",
    "    mniIFG_r = '/home/public/StoppingAge-DWI-Ahead/Data/Atlases/mni_ifg_r_bin.nii.gz'\n",
    "    mnipreSMA_l = '/home/public/StoppingAge-DWI-Ahead/Data/Atlases/mni_presma_l_bin.nii.gz'\n",
    "    mnipreSMA_r = '/home/public/StoppingAge-DWI-Ahead/Data/Atlases/mni_presma_r_bin.nii.gz'\n",
    "    \n",
    "    # Create mean B0 image\n",
    "    # Location of the local copy of the DWI data\n",
    "    inputData = preprocessed_dir+'/'+subject+'_ses-3_dwi-preproc.mif.gz'\n",
    "    \n",
    "    # 1a) First create the mean B0 images\n",
    "    command = 'dwiextract '+inputData+' - -bzero | mrmath - mean '+preprocessed_dir+'/mean_b0.mif -axis 3 -force'\n",
    "    print(command+'\\n')\n",
    "    try:\n",
    "         subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "        raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "    # 1b) Convert the mean B0 mif to a nifti \n",
    "    command = 'mrconvert '+preprocessed_dir+'/mean_b0.mif '+preprocessed_dir+'/mean_b0.nii.gz -force'\n",
    "    print(command+'\\n')\n",
    "    try:\n",
    "         subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "        raise subprocess.CalledProcessError(msg)  \n",
    "        \n",
    "    # Assign Mean B0 to variable:    \n",
    "    meanB0Image = preprocessed_dir+'/mean_b0.nii.gz'\n",
    "    \n",
    "    # 1) Estimate deformation field MNI (moving) -> B0 (fixed) using ANTs\n",
    "    #. We are going to do this in two stages as we found out that this seemed to improve the results\n",
    "    #. Specifically first we are using the standard SyN and then improve the registration by using SyNAggro\n",
    "    #. “SyN”: Symmetric normalization: Affine + deformable transformation, with mutual information as optimization metric.\n",
    "    #. “SyNAggro”: SyN, but with more aggressive registration (fine-scale matching and more deformation)\n",
    "    registrationSteps = [1, 2]\n",
    "    for registrationStep in registrationSteps:\n",
    "        if registrationStep == 1:\n",
    "            fixed = ants.image_read(meanB0Image)\n",
    "            moving = ants.image_read(mniTemplate1mmBrain)\n",
    "            #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "\n",
    "            mytx1 = ants.registration(fixed = fixed , moving = moving, type_of_transform = 'SyN', write_composite_transform = True)\n",
    "            warped_moving = mytx1['warpedmovout']\n",
    "            #fixed.plot(overlay = warped_moving, title = 'After First Registration', overlay_alpha = 0.5)\n",
    "            \n",
    "        if registrationStep == 2:\n",
    "            fixed = ants.image_read(meanB0Image)\n",
    "            moving = mytx1['warpedmovout']\n",
    "            #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "\n",
    "            mytx2 = ants.registration(fixed = fixed , moving = moving, type_of_transform = 'SyNAggro', write_composite_transform = True)\n",
    "            warped_moving = mytx2['warpedmovout']\n",
    "            #fixed.plot(overlay = warped_moving, title = 'After Second Registration', overlay_alpha = 0.5)\n",
    "            \n",
    "    # Check if the direct registration worked\n",
    "    fixed = ants.image_read(meanB0Image)\n",
    "    moving = ants.image_read(mniTemplate1mmBrain)\n",
    "    fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "    warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx1['fwdtransforms'], mytx2['fwdtransforms']])\n",
    "    fixed.plot(overlay = warpedimage, title = 'After Direct Registration', overlay_alpha = 0.5)\n",
    "    \n",
    "    # 2) Apply the two transformation fields to the MNI masks\n",
    "    maskList = [mniIFG_l, mniIFG_r, mnipreSMA_l, mnipreSMA_r]\n",
    "    for mask in maskList:\n",
    "        \n",
    "        inputMask = mask\n",
    "        outputName = 'B0_'+inputMask.split('/')[-1]\n",
    "        outputImageName = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/'+outputName\n",
    "    \n",
    "        fixed = ants.image_read(meanB0Image)\n",
    "        moving = ants.image_read(inputMask)\n",
    "        \n",
    "        #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "        warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx1['fwdtransforms'], mytx2['fwdtransforms']])\n",
    "        #fixed.plot(overlay = warpedimage, title = 'After Registration', overlay_alpha = 0.5)\n",
    "        \n",
    "        # 3) Save the ants image to nifti\n",
    "        # Export the ANTSimage to a nifti via nibabel\n",
    "        fixedNibabel = ants.to_nibabel(fixed)\n",
    "        warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "        # Where I need to copy the affine matrix of the B0 image because the warpfield does something weird with the matrix\n",
    "        #. which then interfers with mrview\n",
    "        mask2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "        mask2Nifti.to_filename(outputImageName)\n",
    "    ###############################\n",
    "    # For a few subjects the SyNAggro did not result in satisfactory registrations.\n",
    "    #. Therefore we reran those but now using SyNBoldAff instead. This was after \n",
    "    #.  testing a number of different methods  \n",
    "    #. \n",
    "    # 1) MNI masks to B0 space redo \n",
    "    ###############################    \n",
    "    subjectsRedo = ['sub-060', 'sub-091']\n",
    "    if subject in subjectsRedo:\n",
    "        # 1) Estimate deformation field MNI (moving) -> B0 (fixed) using ANTs\n",
    "        fixed = ants.image_read(meanB0Image)\n",
    "        moving = ants.image_read(mniTemplate1mmBrain)\n",
    "        \n",
    "        fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "        mytx1 = ants.registration(fixed = fixed , moving = moving, type_of_transform = 'SyNBoldAff', write_composite_transform = True)\n",
    "        warped_moving = mytx1['warpedmovout']\n",
    "        fixed.plot(overlay = warped_moving, title = 'After single stage Registration', overlay_alpha = 0.5)\n",
    "\n",
    "        # 2) Apply the transformation field to the MNI masks\n",
    "        maskList = [mniIFG_l, mniIFG_r, mnipreSMA_l, mnipreSMA_r]\n",
    "        for mask in maskList:\n",
    "\n",
    "            inputMask = mask\n",
    "            outputName = 'B0_'+inputMask.split('/')[-1]\n",
    "            outputImageName = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/'+outputName\n",
    "\n",
    "            fixed = ants.image_read(meanB0Image)\n",
    "            moving = ants.image_read(inputMask)\n",
    "\n",
    "            #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "            warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=mytx1['fwdtransforms'])#, imagetype=3)\n",
    "            #fixed.plot(overlay = warpedimage, title = 'After Registration', overlay_alpha = 0.5)\n",
    "\n",
    "            # 3) Save the ants image to nifti\n",
    "            # Export the ANTSimage to a nifti via nibabel\n",
    "            fixedNibabel = ants.to_nibabel(fixed)\n",
    "            warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "            # Where I need to copy the affine matrix of the B0 image because the warpfield does something weird with the matrix\n",
    "            #. which then interfers with mrview\n",
    "            mask2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "            mask2Nifti.to_filename(outputImageName)\n",
    "        \n",
    "###############################\n",
    "# 2) MASSP masks to B0 space\n",
    "###############################\n",
    "###############################\n",
    "# 2a) First get the MASSP masks and threshold them based on the robust range of 50% of the probability values\n",
    "#. in the mask and binarize them:\n",
    "###############################\n",
    "for subject in subjects:\n",
    "    print(subject + ' MASSP Masks')\n",
    "    # The MASSP files:\n",
    "    labelImage = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/*_massp-label.nii.gz'\n",
    "    probImage = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/*_massp-proba.nii.gz'\n",
    "    \n",
    "    # The resulting seperate subcortical labels\n",
    "    stn_l_label = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/stn_l_label.nii.gz'\n",
    "    stn_r_label = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/stn_r_label.nii.gz'\n",
    "     \n",
    "    stn_l_proba = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/stn_l_proba.nii.gz'\n",
    "    stn_r_proba = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/stn_r_proba.nii.gz'\n",
    "     \n",
    "    stn_l_mask = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/stn_l_masks.nii.gz'\n",
    "    stn_r_mask = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/stn_r_masks.nii.gz'\n",
    "\n",
    "    maskLabels = [stn_l_label, stn_r_label]\n",
    "    maskProba = [stn_l_proba, stn_r_proba]\n",
    "    maskNames = [stn_l_mask, stn_r_mask]\n",
    "    \n",
    "    # Split the 3D label file into its different masks based on the intensity value\n",
    "    #.  Where stn l is 3 and stn r is equal to 4 \n",
    "    #.   so don't change the order of maskNames\n",
    "    for i in (3,4):\n",
    "        command = 'fslmaths '+labelImage+' -thr '+str(i)+' -uthr '+str(i)+' '+maskLabels[i-3] \n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "    # Get the probability values in a mask\n",
    "    for i in (3,4):\n",
    "        command = 'fslmaths '+probImage+' -mas '+maskLabels[i-3]+' '+maskProba[i-3]\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "    # Use robust range \n",
    "    for i in (3,4):\n",
    "        command = 'fslmaths '+maskProba[i-3]+' -thrp 0.05 '+maskProba[i-3]\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "    # Binarize the masks (you can combine the statements but I rather check the intermediate steps)\n",
    "    for i in (3,4):\n",
    "        command = 'fslmaths '+maskProba[i-3]+' -bin '+maskNames[i-3]\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "###############################\n",
    "# Step 2b) register the whole brain mp2rage to B0 and apply transformation matrix to the binarized masks\n",
    "###############################\n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    \n",
    "    # Location of the original T1 data, ideally from the first session, if not available, use the second session:\n",
    "    #. NOTE that we are getting the data from the AHEAD BIDS data folder so that we don't need to create a local \n",
    "    #. copy.\n",
    "    origT1DataSES1 = preprocessedDataFolder+subject+'/ses-1/anat/wb/bet/'+subject+'_ses-1_acq-wb_mod-t1w_orient-std_brain.nii.gz'\n",
    "    origT1DataSES2 = preprocessedDataFolder+subject+'/ses-2/anat/wb/bet/'+subject+'_ses-2_acq-wb_mod-t1w_orient-std_brain.nii.gz'\n",
    "    if (os.path.exists(origT1DataSES1)):\n",
    "        print('Using the 7T scan of the first session\\n')\n",
    "        origT1Data = origT1DataSES1\n",
    "        if subject == 'sub-070':\n",
    "            origT1Data = origT1DataSES2 # First session of this subject is incorrect and you must use ses2\n",
    "    else:\n",
    "        print('Using the 7T scan of the SECOND session\\n')\n",
    "        origT1Data = origT1DataSES2\n",
    "        \n",
    "    # meanB0Image\n",
    "    meanB0Image = preprocessed_dir+'/mean_b0.nii.gz'\n",
    "    \n",
    "    # 1) Estimate deformation field using ANTs T1 (moving) -> mean B0 (fixed)\n",
    "    #. \n",
    "    #. We are going to do this in two stages as we found out that this seemed to improve the results\n",
    "    #. Specifically first we are using the standard DenseRigid and then improve the registration by using SyNAggro\n",
    "    #. “DenseRigid”: Rigid transformation: Only rotation and translation. Employs dense sampling during metric estimation.’\n",
    "    #. “SyNRA”: Symmetric normalization: Rigid + Affine + deformable transformation, with mutual information as optimization metric.\n",
    "    \n",
    "    registrationSteps = [1, 2]\n",
    "    for registrationStep in registrationSteps:\n",
    "        if registrationStep == 1:\n",
    "            fixed = ants.image_read(meanB0Image)\n",
    "            moving = ants.image_read(origT1Data)\n",
    "            #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "            mytx3 = ants.registration(fixed = fixed , moving = moving, type_of_transform = 'DenseRigid', write_composite_transform = True)\n",
    "            warped_moving = mytx3['warpedmovout']\n",
    "            #fixed.plot(overlay = warped_moving, title = 'After First Registration', overlay_alpha = 0.5)\n",
    "            \n",
    "        if registrationStep == 2:\n",
    "            fixed = ants.image_read(meanB0Image)\n",
    "            moving = mytx3['warpedmovout']\n",
    "            #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "            mytx4 = ants.registration(fixed = fixed , moving = moving, type_of_transform = 'SyNRA', write_composite_transform = True)\n",
    "            warped_moving = mytx4['warpedmovout']\n",
    "            #fixed.plot(overlay = warped_moving, title = 'After Second Registration', overlay_alpha = 0.5)\n",
    "    \n",
    "    # Check if the direct registration worked\n",
    "    fixed = ants.image_read(meanB0Image)\n",
    "    moving = ants.image_read(origT1Data)\n",
    "    fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "    warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx3['fwdtransforms'], mytx4['fwdtransforms']])\n",
    "    fixed.plot(overlay = warpedimage, title = 'After Direct Registration', overlay_alpha = 0.5)\n",
    "    \n",
    "    # Apply to masks\n",
    "    for mask in maskNames:\n",
    "        inputMask = mask\n",
    "        outputName = 'B0_'+inputMask.split('/')[-1]\n",
    "        outputImageName = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/'+outputName\n",
    "    \n",
    "        fixed = ants.image_read(meanB0Image)\n",
    "        moving = ants.image_read(inputMask)\n",
    "        \n",
    "        #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "        warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx3['fwdtransforms'], mytx4['fwdtransforms']])\n",
    "        #fixed.plot(overlay = warpedimage, title = 'After Registration', overlay_alpha = 0.5)\n",
    "\n",
    "        # 3) Save the ants image to nifti\n",
    "        # Export the ANTSimage to a nifti via nibabel\n",
    "        fixedNibabel = ants.to_nibabel(fixed)\n",
    "        warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "        # Where I need to copy the affine matrix of the B0 image because the warpfield does something weird with the matrix\n",
    "        #. which then interfers with mrview\n",
    "        mask2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "        mask2Nifti.to_filename(outputImageName)\n",
    "    \n",
    "    \n",
    "###############################        \n",
    "# Step 3) Binarize all the masks in B0 space and combine them in a single nifti\n",
    "###############################\n",
    "for subject in subjects:\n",
    "    print(subject + ' Combine MNI and MASSP masks')\n",
    "    \n",
    "    stn_l_b0_mask = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/B0_stn_l_masks.nii.gz'\n",
    "    stn_r_b0_mask = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/B0_stn_r_masks.nii.gz'\n",
    "    ifg_l_b0_mask = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/B0_mni_ifg_l_bin.nii.gz'\n",
    "    ifg_r_b0_mask = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/B0_mni_ifg_r_bin.nii.gz'\n",
    "    presma_l_b0_mask = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/B0_mni_presma_l_bin.nii.gz'\n",
    "    presma_r_b0_mask = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/B0_mni_presma_r_bin.nii.gz'\n",
    "    \n",
    "    combinedMaskConnectome = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/B0_corticalSubcorticalROI.nii.gz'\n",
    "    inputConnectome = '/home/public/StoppingAge-DWI-Ahead/Data/Derivatives/'+subject+'/B0_corticalSubcorticalROI.mif'\n",
    "    \n",
    "    b0masks = [stn_l_b0_mask, stn_r_b0_mask, ifg_l_b0_mask, ifg_r_b0_mask, presma_l_b0_mask, presma_r_b0_mask]\n",
    "    \n",
    "    # Binarize the masks and multiple with value to get 6 different mask labels\n",
    "    labelCounter = 1\n",
    "    for b0mask in b0masks:\n",
    "        command = 'fslmaths '+b0mask+' -bin -mul '+str(labelCounter)+' '+b0mask\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "        labelCounter += 1\n",
    "    \n",
    "    # Combine the masks in a single nifti\n",
    "    command = 'fslmaths '+b0masks[0]+' -add '+b0masks[1]+' -add '+b0masks[2]+' -add '+b0masks[3]+' -add '+b0masks[4]+' -add '+b0masks[5]+' '+combinedMaskConnectome\n",
    "    subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "   \n",
    "    # Convert the file to mif for mrtrix\n",
    "    command = 'mrconvert '+combinedMaskConnectome+' '+inputConnectome+' -force'\n",
    "    subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "\n",
    "# All combined mask files have been checked in MRview while overlaying it with the \n",
    "#. mean B0 image. All masks seemed to be located in a plausible location.\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84857e-11c3-45f2-903a-e226a010f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 7) Generate the connectome \n",
    "#################################################################################\n",
    "#. Here we want to extract the number of streamlines between the ROI's that are\n",
    "#. weighted based on the SIFT2 and normalized for node volume. Specifically \n",
    "#. the contribution to the connectome edge is scaled by the inverse of the two\n",
    "#. node volumes. \n",
    "#. \n",
    "# Run for al 49 subjects? Yes\n",
    "# Do you want to overwrite the previous results? If yes, then True\n",
    "overwrite = False\n",
    "\n",
    "def genConnectome(subject):\n",
    "    print(subject+'\\n')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    dwiresponse_dir = work_dir+'/dwi2response'\n",
    "    wholebrainStreamline_dir = work_dir+'/tckgen'\n",
    "    sift2_dir = work_dir+'/SIFT2-tckgen'\n",
    "    connectome_dir = work_dir+'/tck2connectome'\n",
    "    \n",
    "    # ROI \n",
    "    connectomeNodes_dir = mainProjectFolder+dwiDerivativeFolder+subject\n",
    "    \n",
    "    # Create the output folder\n",
    "    if ((os.path.exists(connectome_dir)) & (overwrite == False)):\n",
    "        print('\\n4. Generate connectome: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n4. Generate connectome')\n",
    "        os.makedirs(connectome_dir,exist_ok=True)\n",
    "        \n",
    "        command = 'tck2connectome '+wholebrainStreamline_dir+'/tracks_combined.tck '+connectomeNodes_dir+'/B0_corticalSubcorticalROI.mif '+connectome_dir+'/connectome.csv -tck_weights_in '+sift2_dir+'/sift.txt -out_assignments '+connectome_dir+'/assignments.txt -scale_invnodevol -zero_diagonal -force'\n",
    "        \n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "             subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "# Run the actual function   \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(1) \n",
    "    pool.map(genConnectome, subjects) \n",
    "    pool.close()            \n",
    "#################################################################################           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a49bc-b879-48e1-96fb-48d10a2f570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 8) Extract the streamlines between nodes\n",
    "#################################################################################\n",
    "#. Here we want to extract the streamlines so that we can overlay the resulting \n",
    "#. tck images with the B0 and node image to make sure that the streamlines make \n",
    "#. somewhat anatomical sense. Note that we are not using the option 'exclusive'\n",
    "#. as a lot of voxels are shared between the striatum and stn towards the cortex\n",
    "#. \n",
    "# Have you done it for all 49 subjects? Yes\n",
    "# Do you want to overwrite the previous results? If yes, then True\n",
    "overwrite = False\n",
    "\n",
    "def extractStreamlinesPerNodePair(subject):\n",
    "    print(subject+'\\n')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    dwiresponse_dir = work_dir+'/dwi2response'\n",
    "    wholebrainStreamline_dir = work_dir+'/tckgen'\n",
    "    sift2_dir = work_dir+'/SIFT2-tckgen'\n",
    "    connectome_dir = work_dir+'/tck2connectome'\n",
    "    tract_dir = work_dir+'/connectome2tck'\n",
    "    \n",
    "    # ROI \n",
    "    connectomeNodes_dir = mainProjectFolder+dwiDerivativeFolder+subject\n",
    "    \n",
    "    # Create the output folder\n",
    "    if ((os.path.exists(tract_dir)) & (overwrite == False)):\n",
    "        print('\\n4. Extract streamlines from connectome: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n4. Extract streamlines from connectome')\n",
    "        os.makedirs(tract_dir,exist_ok=True)\n",
    "        \n",
    "        command = 'connectome2tck -tck_weights_in '+sift2_dir+'/sift.txt -prefix_tck_weights_out '+sift2_dir+'/tckweights_ '+wholebrainStreamline_dir+'/tracks_combined.tck '+connectome_dir+'/assignments.txt '+tract_dir+'/egde- -force'\n",
    "                \n",
    "        print(command+'\\n')\n",
    "        try:\n",
    "             subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "# Run the actual function   \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(6)\n",
    "    pool.imap(extractStreamlinesPerNodePair, subjects) \n",
    "    pool.close()            \n",
    "#################################################################################            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb391e7-640f-49d6-a33a-15a50c671405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################   \n",
    "# 9) Count the number of streamlines for each track\n",
    "################################################################################# \n",
    "# Originally we were doing this using tckstats count, but this seemed to just count the number of rows\n",
    "# in each array rather than summing their value. Therefore to count the number of streamlines for each \n",
    "# track, we're just using numpy.sum on each of the csv fiules we just generated using connectome2tck\n",
    "\n",
    "\n",
    "listOflists = []\n",
    "\n",
    "\n",
    "print('\\nHow many subjects do we have? ', len(subjects))\n",
    "for subject in subjects:\n",
    "    print(subject)\n",
    "    \n",
    "    # Local path \n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    sift2_dir = work_dir+'/SIFT2-tckgen'\n",
    "\n",
    "    # meaningful labels to give the final text file \n",
    "    hemisphereNames= np.repeat(['left', 'right'],3)\n",
    "    anatomicalPairNames = ['STN-IFG', 'STN-preSMA', 'IFG-preSMA', 'STN-IFG', 'STN-preSMA', 'IFG-preSMA']\n",
    "    \n",
    "    # tracts - sum the tracts that are listed here \n",
    "    # the order of this list needs to match hemisphereNames and ananatomicalPairNames\n",
    "    coordinatePairs = ['1-3', '1-5', '3-5', '2-4', '2-6', '4-6'] \n",
    "    \n",
    "    for (hemisphere, ROI, track) in zip(hemisphereNames, anatomicalPairNames, coordinatePairs):\n",
    "        readfile = np.array(pd.read_csv(sift2_dir+'/tckweights_'+track+'.csv', header = None))\n",
    "        sumofweights = np.sum(readfile)\n",
    "    \n",
    "        \n",
    "        #write it to a df \n",
    "        row = [subject, hemisphere, ROI, track, sumofweights]\n",
    "        listOflists.append(row)\n",
    "\n",
    "dfCombined = pd.DataFrame(listOflists, columns = ['subjectID', 'hemisphere', 'ROI', 'trackID', 'streamlineCount'])\n",
    "dfCombined.to_csv(mainProjectFolder+dwiDataFolder+'sumofWeightedTracks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224805f-5aec-4a00-a794-1535e7722564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 10) Code to register the mean TCK streamlines to MNI\n",
    "#################################################################################\n",
    "overwrite = True\n",
    "\n",
    "for subject in subjects:\n",
    "    print(subject+' - generate nii track images in B0 space + B0 to MNI')\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    preprocessed_dir = work_dir+'/preprocessed'\n",
    "    tract_dir = work_dir+'/connectome2tck'\n",
    "    mni_dir = work_dir+'/mni'\n",
    "    weighted_dir = work_dir+'/tck-weighted'\n",
    "    \n",
    "    # Create the output folder\n",
    "    if ((os.path.exists(mni_dir)) & (overwrite == False)):\n",
    "        print('\\nTracks already generated and mean B0 registered to MNI (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\nGenerate track-weighted files in individual space and register mean B0 to MNI space')\n",
    "        os.makedirs(mni_dir, exist_ok=True)\n",
    "        os.makedirs(weighted_dir, exist_ok=True)\n",
    "\n",
    "        # Location of the MNI template and masks data:\n",
    "        mniTemplate1mmBrain = '/home/public/StoppingAge-DWI-Ahead/Data/Atlases/MNI152_T1_1mm_brain.nii.gz'\n",
    "        STN_IFG_L_tck = tract_dir+'/egde-1-3.tck'\n",
    "        STN_IFG_R_tck = tract_dir+'/egde-2-4.tck'\n",
    "        STN_preSMA_L_tck = tract_dir+'/egde-1-5.tck'\n",
    "        STN_preSMA_R_tck = tract_dir+'/egde-2-6.tck'\n",
    "        IFG_preSMA_L_tck = tract_dir+'/egde-3-5.tck'\n",
    "        IFG_preSMA_R_tck = tract_dir+'/egde-4-6.tck'\n",
    "        \n",
    "        STN_IFG_L_nii = weighted_dir+'/egde-1-3.nii.gz'\n",
    "        STN_IFG_R_nii = weighted_dir+'/egde-2-4.nii.gz'\n",
    "        STN_preSMA_L_nii = weighted_dir+'/egde-1-5.nii.gz'\n",
    "        STN_preSMA_R_nii = weighted_dir+'/egde-2-6.nii.gz'\n",
    "        IFG_preSMA_L_nii = weighted_dir+'/egde-3-5.nii.gz'\n",
    "        IFG_preSMA_R_nii = weighted_dir+'/egde-4-6.nii.gz'\n",
    "        \n",
    "        tracts_tck = [STN_IFG_L_tck, STN_IFG_R_tck, STN_preSMA_L_tck, STN_preSMA_R_tck, IFG_preSMA_L_tck, IFG_preSMA_R_tck]\n",
    "        tracts_nii = [STN_IFG_L_nii, STN_IFG_R_nii, STN_preSMA_L_nii, STN_preSMA_R_nii, IFG_preSMA_L_nii, IFG_preSMA_R_nii]\n",
    "        \n",
    "        # Assign Mean B0 to variable:    \n",
    "        meanB0Image = preprocessed_dir+'/mean_b0.nii.gz'\n",
    "        \n",
    "        # Transform the TCK files to nii.gz format\n",
    "        for i in range(0, len(tracts_tck)):\n",
    "            command =  'tckmap -template '+meanB0Image+' '+tracts_tck[i]+' '+tracts_nii[i]+' -force'\n",
    "            print(command)\n",
    "            try:\n",
    "                 subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "                raise subprocess.CalledProcessError(msg)\n",
    "        \n",
    "        # Register the B0 to MNI space and apply it to the tracts\n",
    "        # 1) Determine the transformation fields\n",
    "        registrationSteps = [1, 2]\n",
    "        for registrationStep in registrationSteps:\n",
    "            if registrationStep == 1:\n",
    "                fixed = ants.image_read(mniTemplate1mmBrain)\n",
    "                moving = ants.image_read(meanB0Image)\n",
    "\n",
    "                #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "                mytx1 = ants.registration(fixed = fixed , moving = moving, type_of_transform = 'SyN', write_composite_transform = True)\n",
    "                warped_moving = mytx1['warpedmovout']\n",
    "                #fixed.plot(overlay = warped_moving, title = 'After First Registration', overlay_alpha = 0.5)\n",
    "\n",
    "            if registrationStep == 2:\n",
    "                fixed = ants.image_read(mniTemplate1mmBrain)\n",
    "                moving = mytx1['warpedmovout']\n",
    "\n",
    "                #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "                mytx2 = ants.registration(fixed = fixed, moving = moving, type_of_transform = 'SyNAggro', write_composite_transform = True)\n",
    "                warped_moving = mytx2['warpedmovout']\n",
    "                #fixed.plot(overlay = warped_moving, title = 'After Second Registration', overlay_alpha = 0.5)\n",
    "\n",
    "        # Check if the direct registration worked\n",
    "        fixed = ants.image_read(mniTemplate1mmBrain)\n",
    "        moving = ants.image_read(meanB0Image)\n",
    "\n",
    "        fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "        warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx1['fwdtransforms'], mytx2['fwdtransforms']])\n",
    "        fixed.plot(overlay = warpedimage, title = 'After Direct Registration', overlay_alpha = 0.5)\n",
    "\n",
    "        # 2) Apply the two transformation fields to the MNI masks\n",
    "        for mask in tracts_nii:\n",
    "            print(mask)\n",
    "            inputMask = mask\n",
    "            #outputName = 'MNI1mm_'+inputMask.split('/')[-1]\n",
    "            outputImageName = mni_dir+'/MNI1mm_'+subject+'_'+inputMask.split('/')[-1]\n",
    "            print(outputImageName)\n",
    "            fixed = ants.image_read(mniTemplate1mmBrain)\n",
    "            moving = ants.image_read(inputMask)\n",
    "            print(mniTemplate1mmBrain)\n",
    "            #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.5)\n",
    "            warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx1['fwdtransforms'], mytx2['fwdtransforms']])\n",
    "            #fixed.plot(overlay = warpedimage, title = 'After Registration', overlay_alpha = 0.5)\n",
    "\n",
    "            # 3) Save the ants image to nifti\n",
    "            # Export the ANTSimage to a nifti via nibabel\n",
    "            fixedNibabel = ants.to_nibabel(fixed)\n",
    "            warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "            # Where I need to copy the affine matrix of the B0 image because the warpfield does something weird with the matrix\n",
    "            #. which then interfers with mrview\n",
    "            mask2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "            mask2Nifti.to_filename(outputImageName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b811a3-dc35-4cdb-98aa-ed536627c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#    FA    #\n",
    "############\n",
    "\n",
    "# This creates an FA map using ODFs (or FODs), as opposed to using the traditional diffusion tensor. It is thus \n",
    "# better able to take into account crossing fibres. From Tan et al 2014 (doi: 10.1002/jmri.24589) \n",
    "\n",
    "\n",
    "overwrite = False \n",
    "\n",
    "\n",
    "def generalisedFA(subject):\n",
    "    print(subject+'\\n')\n",
    "    \n",
    "    # set directories \n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    fod_dir = work_dir+'/dwi2fod'\n",
    "    \n",
    "    gfa = fod_dir+'/newFA.nii.gz'\n",
    "    if ((os.path.exists(gfa)) & (overwrite == False)):\n",
    "        print('GFA already exists for ' +subject+'\\n') \n",
    "    else:\n",
    "        print('Creating GFA for '+subject+'\\n')\n",
    "\n",
    "        # load the wm fod\n",
    "        wmfod = nib.load(fod_dir+'/wmfod.nii.gz')\n",
    "    \n",
    "        fmean = np.mean(np.abs(wmfod.get_fdata()))\n",
    "        gfa = np.sum(45*(np.abs(wmfod.get_fdata())-fmean)**2,axis=3)/np.maximum(0.00000001,(44*np.sum(wmfod.get_fdata()**2,axis=3)))\n",
    "        gfa = np.minimum(gfa,1.0)\n",
    "    \n",
    "        img = nib.Nifti1Image(gfa,wmfod.affine,wmfod.header)\n",
    "        nib.save(img, gfa)\n",
    "\n",
    "      \n",
    "    \n",
    "# Run the function \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(2) \n",
    "    pool.map(generalisedFA, subjects) \n",
    "    pool.close() \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255207fa-794e-41e8-bb93-318504dd81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "#     basic ADC   #\n",
    "##############\n",
    "\n",
    "# Like with the FA, we wanted to create a more sophisticated estimation of anisotropy than the standard \n",
    "# MA that is calculated using a tensor. Calculated by combining the three ODF maps (one each for grey matter, \n",
    "# white matter, and CSF). From Volz et al 2018 (doi: 10.1007/s00429-017-1508-x)\n",
    "\n",
    "# NOTE from Pilou: this ODF component does not measure what I thought it did :(\n",
    "# We need instead to revert to the classical definition of ADC, based on averaging the dwi data per shell\n",
    "# and taking ADC = -1/Bval*log(mean_Bval/mean_B0)\n",
    "\n",
    "\n",
    "overwrite = False \n",
    "\n",
    "\n",
    "def createBasicADC(subject):\n",
    "   \n",
    "    #local path settings \n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    dwi_dir = work_dir+'/preprocessed'\n",
    "    fod_dir = work_dir+'/dwi2fod'\n",
    "    \n",
    "    # ADC file location - check if this exists first, and create it if not \n",
    "    adc = fod_dir+'/adc.nii.gz'\n",
    "\n",
    "    if ((os.path.exists(adc)) & (overwrite == False)):\n",
    "        print('Basic ADC already exists for ' +subject+'\\n') \n",
    "    else:\n",
    "        print('Creating basic ADC for '+subject+'\\n')\n",
    "        \n",
    "        dwi_img = dwi_dir+'/'+subject+'_ses-3_dwi-preproc.mif.gz'\n",
    "        # compute the average for each shell\n",
    "        b0_img = fod_dir+'/'+subject+'mean_b0.nii.gz'\n",
    "        command =  'dwiextract '+dwi_img+' - -shells 0 | mrmath - mean '+b0_img+' -axis 3 -force'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)# get the three FOD files \n",
    "        \n",
    "        b700_img = fod_dir+'/'+subject+'mean_b700.nii.gz'\n",
    "        command =  'dwiextract '+dwi_img+' - -shells 700 | mrmath - mean '+b700_img+' -axis 3 -force'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)# get the three FOD files \n",
    "        \n",
    "        b1000_img = fod_dir+'/'+subject+'mean_b1000.nii.gz'\n",
    "        command =  'dwiextract '+dwi_img+' - -shells 1000 | mrmath - mean '+b1000_img+' -axis 3 -force'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)# get the three FOD files \n",
    "        \n",
    "        b1600_img = fod_dir+'/'+subject+'mean_b1600.nii.gz'\n",
    "        command =  'dwiextract '+dwi_img+' - -shells 1600 | mrmath - mean '+b1600_img+' -axis 3 -force'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)# get the three FOD files \n",
    "        \n",
    "        b0 = nib.load(b0_img)\n",
    "        b700 = nib.load(b700_img)\n",
    "        b1000 = nib.load(b1000_img)\n",
    "        b1600 = nib.load(b1600_img)\n",
    "        \n",
    "        # now take the first volume of each and combine them \n",
    "        adc = (-1.0/700.0*np.log(b700.get_fdata()/b0.get_fdata(),where=(b700.get_fdata()*b0.get_fdata()>0)) \\\n",
    "               -1.0/1000.0*np.log(b1000.get_fdata()/b0.get_fdata(),where=(b1000.get_fdata()*b0.get_fdata()>0)) \\\n",
    "               -1.0/1600.0*np.log(b1600.get_fdata()/b0.get_fdata(),where=(b1600.get_fdata()*b0.get_fdata()>0)) \\\n",
    "              )/3.0\n",
    "        \n",
    "        # save the new file with affine and header info and save\n",
    "        img = nib.Nifti1Image(adc,b0.affine,b0.header) \n",
    "        nib.save(img, fod_dir+'/adc.nii.gz')\n",
    "            \n",
    "        \n",
    "        \n",
    "# run function        \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(4) \n",
    "    pool.map(createBasicADC, subjects) \n",
    "    pool.close() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06a8a5-b5ad-4e89-aabb-6d0c2e52a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################   \n",
    "# 13. Get the average FA and MD values and export to a CSV\n",
    "################################################################################# \n",
    "\n",
    "listOflists = [] \n",
    "\n",
    "print('\\nNumber of subjects is: ', len(subjects))\n",
    "\n",
    "for subject in subjects:\n",
    "    print(subject)\n",
    "    \n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject    \n",
    "    track_dir = work_dir+'/tck-weighted'\n",
    "    fod_dir = work_dir+'/dwi2fod'\n",
    "\n",
    "    # get the tract images for each participant \n",
    "    listOftracts = glob.glob(track_dir+'/*.gz')\n",
    "    listOftracts.sort()\n",
    "           \n",
    "    # Get the mean FA and MD per tract \n",
    "    for track in listOftracts:\n",
    "        # fa\n",
    "        facommand = 'mrstats -output mean -mask '+track + \" \" + fod_dir+'/newFA.nii.gz'\n",
    "\n",
    "        try:\n",
    "            out = subprocess.check_output(facommand, shell=True, universal_newlines=True)\n",
    "            fa_value =  float(re.search(r'\\d+\\.\\d+', out).group())\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "        \n",
    "        # md \n",
    "        mdcommand = 'mrstats -output mean -mask '+track + \" \" + fod_dir+'/adc.nii.gz'\n",
    "        \n",
    "        try:\n",
    "            outmd = subprocess.check_output(mdcommand, shell=True, universal_newlines=True)\n",
    "            md_value =  float(re.search(r'\\d+\\.\\d+', outmd).group())\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "        row = [subject, track, fa_value, md_value]\n",
    "        listOflists.append(row)\n",
    "        \n",
    "        \n",
    "# put together in a nice df    \n",
    "dfCombined = pd.DataFrame(listOflists, columns = ['subjectID', 'file_name', 'fa_value', 'md_value'])\n",
    "\n",
    "\n",
    "# now add columns with more meaningful labels, based on the file names \n",
    "\n",
    "def fullTractNames (s):\n",
    "    if 'egde-1-3' in s: \n",
    "        return 'left STN-IFG'\n",
    "    elif 'egde-1-5' in s: \n",
    "        return 'left STN-preSMA'\n",
    "    elif 'egde-3-5' in s: \n",
    "        return 'left IFG-preSMA'\n",
    "    elif 'egde-2-4' in s: \n",
    "        return 'right STN-IFG'\n",
    "    elif 'egde-2-6' in s: \n",
    "        return 'right STN-preSMA'\n",
    "    elif 'egde-4-6' in s: \n",
    "        return 'right IFG-preSMA'\n",
    "    \n",
    "def ROInames (s):\n",
    "    if 'egde-1-3' in s or 'egde-2-4' in s:\n",
    "        return 'STN-IFG'\n",
    "    elif 'egde-1-5' in s or 'egde-2-6' in s:\n",
    "        return 'STN-preSMA'\n",
    "    elif 'egde-3-5' in s or 'egde-4-6' in s:\n",
    "        return 'IFG-preSMA'\n",
    "    \n",
    "def hemisphereNames (s):\n",
    "    if 'egde-1' in s or 'egde-3' in s:\n",
    "        return 'left'\n",
    "    elif 'egde-2' in s or 'egde-4' in s:\n",
    "        return 'right'\n",
    "    \n",
    "dfCombined['tract'] = dfCombined['file_name'].apply(fullTractNames)\n",
    "dfCombined['hemisphere'] = dfCombined['file_name'].apply(hemisphereNames)\n",
    "dfCombined['ROI'] = dfCombined['file_name'].apply(ROInames)\n",
    "\n",
    "dfCombined.to_csv(mainProjectFolder+dwiDataFolder+'updated_diffusion_values.csv')\n",
    "\n",
    "\n",
    "################################################################################# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c775c57-847b-42f7-b128-41c6a82acc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################   \n",
    "# 14. Get the average iron and myelin value for each participant per tract \n",
    "################################################################################# \n",
    "listOflists = [] \n",
    "\n",
    "print('\\nNumber of subjects is: ', len(subjects))\n",
    "for subject in subjects:\n",
    "    print(subject)\n",
    "    # Local path settings\n",
    "    work_dir = mainProjectFolder+dwiDataFolder+subject\n",
    "    qmri_dir = work_dir+'/qmri_est'        \n",
    "    track_dir = work_dir+'/tck-weighted'\n",
    "\n",
    "\n",
    "    # get the tract images for each participant \n",
    "    listOftracts = glob.glob(track_dir+'/*.gz')\n",
    "    listOftracts.sort()\n",
    "    \n",
    "    # get the iron image\n",
    "    # some of the participants have ses_2 in their filenames so finding the file this way \n",
    "    ironimg = glob.glob(qmri_dir+'/*map-iron_def*')\n",
    "    \n",
    "    # get the myelin image\n",
    "    myelinimg = glob.glob(qmri_dir+'/*myelin_ants-def*')\n",
    "    \n",
    "       \n",
    "    # Get the mean iron and myelin per tract\n",
    "    for track in listOftracts:\n",
    "        meaniron = 'mrstats -output mean -mask '+track+' '+ironimg[0]\n",
    "        print(meaniron)\n",
    "\n",
    "        try:\n",
    "            out = subprocess.check_output(meaniron, shell=True, universal_newlines=True)\n",
    "            iron_value =  float(re.search(r'\\d+\\.\\d+', out).group())\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "        meanmyelin = 'mrstats -output mean -mask '+track+' '+myelinimg[0]\n",
    "        print(meanmyelin)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            out = subprocess.check_output(meanmyelin, shell=True, universal_newlines=True)\n",
    "            myelin_value =  float(re.search(r'\\d+\\.\\d+', out).group())\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "            \n",
    "  \n",
    "        row = [subject, track, iron_value, myelin_value]\n",
    "        listOflists.append(row)\n",
    "        \n",
    "dfCombined = pd.DataFrame(listOflists, columns = ['subjectID', 'file_name', 'iron_value', 'myelin_value'])\n",
    "\n",
    "# add more meaningful labels (these functions were defined in the previous module so may need to run this first)\n",
    "dfCombined['tract'] = dfCombined['file_name'].apply(fullTractNames)\n",
    "dfCombined['hemisphere'] = dfCombined['file_name'].apply(hemisphereNames)\n",
    "dfCombined['ROI'] = dfCombined['file_name'].apply(ROInames)\n",
    "\n",
    "dfCombined.to_csv(mainProjectFolder+dwiDataFolder+'qmri_values.csv')\n",
    "\n",
    "\n",
    "#################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
